{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12733395,"sourceType":"datasetVersion","datasetId":8048682}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/daily-aqi-malang/air-quality-malang.csv')\ndf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-19T09:10:29.884101Z","iopub.execute_input":"2025-08-19T09:10:29.884692Z","iopub.status.idle":"2025-08-19T09:10:29.917591Z","shell.execute_reply.started":"2025-08-19T09:10:29.884653Z","shell.execute_reply":"2025-08-19T09:10:29.916933Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"            date   pm25\n0       2025/8/1     54\n1       2025/8/3     33\n2       2025/8/4     79\n3       2025/8/5     74\n4       2025/8/6     82\n...          ...    ...\n1269  2021/12/29     29\n1270  2021/12/30     44\n1271  2021/12/31     49\n1272    2022/1/1     54\n1273    2022/1/2     32\n\n[1274 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>pm25</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2025/8/1</td>\n      <td>54</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2025/8/3</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2025/8/4</td>\n      <td>79</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2025/8/5</td>\n      <td>74</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2025/8/6</td>\n      <td>82</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1269</th>\n      <td>2021/12/29</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>1270</th>\n      <td>2021/12/30</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>1271</th>\n      <td>2021/12/31</td>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>1272</th>\n      <td>2022/1/1</td>\n      <td>54</td>\n    </tr>\n    <tr>\n      <th>1273</th>\n      <td>2022/1/2</td>\n      <td>32</td>\n    </tr>\n  </tbody>\n</table>\n<p>1274 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"df_cleaned = df.copy()\ndf_cleaned = df_cleaned.rename(columns=lambda x: x.strip())\ndf_cleaned['date'] = pd.to_datetime(df_cleaned['date'], format='%Y/%m/%d', errors='coerce')\ndf_cleaned = df_cleaned.dropna(subset=['date'])\ndf_cleaned = df_cleaned.sort_values('date').reset_index(drop=True)\ndf_cleaned = df_cleaned.set_index('date')\ndf_cleaned = df_cleaned.resample('D').asfreq()\ndf_cleaned['pm25'] = df_cleaned['pm25'].interpolate(method='linear', limit_direction='both')\ndf_cleaned","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T09:10:29.918752Z","iopub.execute_input":"2025-08-19T09:10:29.919181Z","iopub.status.idle":"2025-08-19T09:10:29.950415Z","shell.execute_reply.started":"2025-08-19T09:10:29.919162Z","shell.execute_reply":"2025-08-19T09:10:29.949738Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"             pm25\ndate             \n2021-12-28   44.0\n2021-12-29   29.0\n2021-12-30   44.0\n2021-12-31   49.0\n2022-01-01   54.0\n...           ...\n2025-08-07  102.0\n2025-08-08  121.0\n2025-08-09  107.0\n2025-08-10   95.0\n2025-08-11   55.0\n\n[1323 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pm25</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2021-12-28</th>\n      <td>44.0</td>\n    </tr>\n    <tr>\n      <th>2021-12-29</th>\n      <td>29.0</td>\n    </tr>\n    <tr>\n      <th>2021-12-30</th>\n      <td>44.0</td>\n    </tr>\n    <tr>\n      <th>2021-12-31</th>\n      <td>49.0</td>\n    </tr>\n    <tr>\n      <th>2022-01-01</th>\n      <td>54.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2025-08-07</th>\n      <td>102.0</td>\n    </tr>\n    <tr>\n      <th>2025-08-08</th>\n      <td>121.0</td>\n    </tr>\n    <tr>\n      <th>2025-08-09</th>\n      <td>107.0</td>\n    </tr>\n    <tr>\n      <th>2025-08-10</th>\n      <td>95.0</td>\n    </tr>\n    <tr>\n      <th>2025-08-11</th>\n      <td>55.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1323 rows × 1 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import os\nimport joblib\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nwarnings.filterwarnings('ignore')\n\n# ================= CONFIG =================\nSEQ_LENGTH = 60\nFUTURE_DAYS = 7\nBATCH_SIZE = 16\nEPOCHS = 200\nLR = 0.001\nMODEL_NAME = \"pm25_lstm_v1\"\n\n# ================= DATA PREP =================\ndef prepare_features(df):\n    df = df.copy()\n    df.index = pd.to_datetime(df.index)\n    df['day_of_week'] = df.index.dayofweek\n    df['month'] = df.index.month\n    df['quarter'] = df.index.quarter\n    df['is_weekend'] = (df.index.dayofweek >= 5).astype(int)\n    df['hour'] = getattr(df.index, 'hour', 0)\n    for lag in [1, 2, 3, 7, 14]:\n        df[f'pm25_lag_{lag}'] = df.iloc[:, 0].shift(lag)\n    for window in [3, 7, 14]:\n        df[f'pm25_rolling_mean_{window}'] = df.iloc[:, 0].rolling(window).mean()\n        df[f'pm25_rolling_std_{window}'] = df.iloc[:, 0].rolling(window).std()\n    df['pm25_pct_change'] = df.iloc[:, 0].pct_change()\n    df['pm25_diff'] = df.iloc[:, 0].diff()\n    return df.dropna()\n\ndef create_sequences(data, seq_length, future_days):\n    X, y = [], []\n    for i in range(len(data) - seq_length - future_days + 1):\n        X.append(data[i:i+seq_length])\n        y.append(data[i+seq_length:i+seq_length+future_days, 0])\n    return np.array(X), np.array(y)\n\n# ================= MODEL =================\ndef build_lstm_model(input_shape):\n    model = Sequential([\n        LSTM(256, return_sequences=True, input_shape=input_shape),\n        Dropout(0.2),\n        BatchNormalization(),\n        LSTM(128, return_sequences=True),\n        Dropout(0.2),\n        BatchNormalization(),\n        LSTM(64, return_sequences=False),\n        Dropout(0.2),\n        Dense(128, activation='relu'),\n        Dropout(0.1),\n        Dense(64, activation='relu'),\n        Dense(FUTURE_DAYS)\n    ])\n    optimizer = Adam(learning_rate=LR, clipnorm=1.0)\n    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n    return model\n\n# ================= TRAIN =================\ndef train_model(X_train, y_train, X_val, y_val, model):\n    callbacks = [\n        EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True, min_delta=0.001),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=1e-7, verbose=1)\n    ]\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        callbacks=callbacks,\n        verbose=1\n    )\n    return history\n\n# ================= EVAL =================\ndef inverse_transform_sequences(pred_seq, scaler, num_features):\n    temp = np.zeros((pred_seq.shape[0], pred_seq.shape[1], num_features))\n    temp[:, :, 0] = pred_seq\n    result = np.zeros_like(pred_seq)\n    for i in range(pred_seq.shape[0]):\n        inv = scaler.inverse_transform(temp[i])\n        result[i] = inv[:, 0]\n    return result\n\ndef calculate_metrics(y_true, y_pred):\n    y_true_flat, y_pred_flat = y_true.flatten(), y_pred.flatten()\n    mae = mean_absolute_error(y_true_flat, y_pred_flat)\n    rmse = np.sqrt(mean_squared_error(y_true_flat, y_pred_flat))\n    mask = y_true_flat != 0\n    mape = np.mean(np.abs((y_true_flat[mask] - y_pred_flat[mask]) / y_true_flat[mask])) * 100\n    r2 = 1 - np.sum((y_true_flat - y_pred_flat)**2) / np.sum((y_true_flat - np.mean(y_true_flat))**2)\n    return mae, rmse, mape, r2\n\n# ================= SAVE =================\ndef save_model_and_components(model, scaler, df_processed, model_name=MODEL_NAME):\n    model_dir = f\"{model_name}_files\"\n    os.makedirs(model_dir, exist_ok=True)\n    model.save(os.path.join(model_dir, f\"{model_name}.h5\"))\n    joblib.dump(scaler, os.path.join(model_dir, f\"{model_name}_scaler.joblib\"))\n    metadata = {\n        'feature_names': df_processed.columns.tolist(),\n        'sequence_length': SEQ_LENGTH,\n        'future_days': FUTURE_DAYS,\n        'num_features': len(df_processed.columns)\n    }\n    joblib.dump(metadata, os.path.join(model_dir, f\"{model_name}_metadata.joblib\"))\n    df_processed.tail(100).to_csv(os.path.join(model_dir, f\"{model_name}_sample_data.csv\"))\n    print(f\"Saved all components to {model_dir}\")\n\n# ================= MAIN =================\ndef main(df_cleaned):\n    df = prepare_features(df_cleaned)\n    scaler = StandardScaler()\n    scaled = scaler.fit_transform(df)\n    X, y = create_sequences(scaled, SEQ_LENGTH, FUTURE_DAYS)\n    train_size, val_size = int(len(X) * 0.7), int(len(X) * 0.15)\n    X_train, y_train = X[:train_size], y[:train_size]\n    X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n    X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n    model = build_lstm_model((SEQ_LENGTH, X.shape[2]))\n    history = train_model(X_train, y_train, X_val, y_val, model)\n    y_pred = model.predict(X_test)\n    y_pred_rescaled = inverse_transform_sequences(y_pred, scaler, X.shape[2])\n    y_test_rescaled = inverse_transform_sequences(y_test, scaler, X.shape[2])\n    mae, rmse, mape, r2 = calculate_metrics(y_test_rescaled, y_pred_rescaled)\n    print(f\"MAE={mae:.2f}, RMSE={rmse:.2f}, MAPE={mape:.2f}%, R²={r2:.4f}\")\n    save_model_and_components(model, scaler, df)\n    return model, history\n\nif __name__ == \"__main__\":\n    model, history = main(df_cleaned)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T09:10:29.951161Z","iopub.execute_input":"2025-08-19T09:10:29.951423Z","iopub.status.idle":"2025-08-19T09:11:24.509978Z","shell.execute_reply.started":"2025-08-19T09:10:29.951400Z","shell.execute_reply":"2025-08-19T09:11:24.509242Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1755594635.907200      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1755594635.907907      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/200\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1755594644.351473      99 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - loss: 0.8548 - mae: 0.7429 - val_loss: 0.9634 - val_mae: 0.7684 - learning_rate: 0.0010\nEpoch 2/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.7089 - mae: 0.6780 - val_loss: 1.0331 - val_mae: 0.7455 - learning_rate: 0.0010\nEpoch 3/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.6576 - mae: 0.6470 - val_loss: 0.9257 - val_mae: 0.7601 - learning_rate: 0.0010\nEpoch 4/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.5865 - mae: 0.6143 - val_loss: 0.8786 - val_mae: 0.7435 - learning_rate: 0.0010\nEpoch 5/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.5441 - mae: 0.5896 - val_loss: 0.8696 - val_mae: 0.7117 - learning_rate: 0.0010\nEpoch 6/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.5090 - mae: 0.5681 - val_loss: 0.9055 - val_mae: 0.7284 - learning_rate: 0.0010\nEpoch 7/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.4915 - mae: 0.5591 - val_loss: 0.8960 - val_mae: 0.7095 - learning_rate: 0.0010\nEpoch 8/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.4915 - mae: 0.5564 - val_loss: 0.8667 - val_mae: 0.7160 - learning_rate: 0.0010\nEpoch 9/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.4677 - mae: 0.5392 - val_loss: 0.8824 - val_mae: 0.7537 - learning_rate: 0.0010\nEpoch 10/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.4457 - mae: 0.5291 - val_loss: 0.8374 - val_mae: 0.7206 - learning_rate: 0.0010\nEpoch 11/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.4134 - mae: 0.5091 - val_loss: 0.9144 - val_mae: 0.7619 - learning_rate: 0.0010\nEpoch 12/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.4150 - mae: 0.5052 - val_loss: 0.9888 - val_mae: 0.8017 - learning_rate: 0.0010\nEpoch 13/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.4103 - mae: 0.5077 - val_loss: 0.9080 - val_mae: 0.7549 - learning_rate: 0.0010\nEpoch 14/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.4128 - mae: 0.5024 - val_loss: 1.0063 - val_mae: 0.7814 - learning_rate: 0.0010\nEpoch 15/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.3893 - mae: 0.4900 - val_loss: 0.9789 - val_mae: 0.7772 - learning_rate: 0.0010\nEpoch 16/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.3610 - mae: 0.4734 - val_loss: 1.1859 - val_mae: 0.8495 - learning_rate: 0.0010\nEpoch 17/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.3680 - mae: 0.4756 - val_loss: 0.9930 - val_mae: 0.7857 - learning_rate: 0.0010\nEpoch 18/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.3639 - mae: 0.4734 - val_loss: 0.9507 - val_mae: 0.7662 - learning_rate: 0.0010\nEpoch 19/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.3420 - mae: 0.4632 - val_loss: 0.9346 - val_mae: 0.7633 - learning_rate: 0.0010\nEpoch 20/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.3573 - mae: 0.4718 - val_loss: 0.8715 - val_mae: 0.7279 - learning_rate: 0.0010\nEpoch 21/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.3368 - mae: 0.4565 - val_loss: 0.8627 - val_mae: 0.7040 - learning_rate: 0.0010\nEpoch 22/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.3220 - mae: 0.4428 - val_loss: 1.1164 - val_mae: 0.8163 - learning_rate: 0.0010\nEpoch 23/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.3200 - mae: 0.4474 - val_loss: 0.9363 - val_mae: 0.7374 - learning_rate: 0.0010\nEpoch 24/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.3334 - mae: 0.4541 - val_loss: 1.0568 - val_mae: 0.7924 - learning_rate: 0.0010\nEpoch 25/200\n\u001b[1m53/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3091 - mae: 0.4363\nEpoch 25: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.3095 - mae: 0.4367 - val_loss: 0.9344 - val_mae: 0.7245 - learning_rate: 0.0010\nEpoch 26/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.2994 - mae: 0.4312 - val_loss: 0.9359 - val_mae: 0.7314 - learning_rate: 5.0000e-04\nEpoch 27/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.2934 - mae: 0.4229 - val_loss: 0.9532 - val_mae: 0.7448 - learning_rate: 5.0000e-04\nEpoch 28/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.2887 - mae: 0.4195 - val_loss: 1.0458 - val_mae: 0.7627 - learning_rate: 5.0000e-04\nEpoch 29/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.2802 - mae: 0.4154 - val_loss: 1.0155 - val_mae: 0.7534 - learning_rate: 5.0000e-04\nEpoch 30/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.2729 - mae: 0.4115 - val_loss: 0.9745 - val_mae: 0.7484 - learning_rate: 5.0000e-04\nEpoch 31/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.2720 - mae: 0.4101 - val_loss: 1.0167 - val_mae: 0.7582 - learning_rate: 5.0000e-04\nEpoch 32/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.2631 - mae: 0.4039 - val_loss: 1.0038 - val_mae: 0.7524 - learning_rate: 5.0000e-04\nEpoch 33/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.2690 - mae: 0.4068 - val_loss: 0.9901 - val_mae: 0.7446 - learning_rate: 5.0000e-04\nEpoch 34/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.2544 - mae: 0.3982 - val_loss: 1.0392 - val_mae: 0.7621 - learning_rate: 5.0000e-04\nEpoch 35/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.2731 - mae: 0.4113 - val_loss: 0.9784 - val_mae: 0.7449 - learning_rate: 5.0000e-04\nEpoch 36/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.2549 - mae: 0.3990 - val_loss: 0.9513 - val_mae: 0.7362 - learning_rate: 5.0000e-04\nEpoch 37/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.2537 - mae: 0.3969 - val_loss: 0.9805 - val_mae: 0.7443 - learning_rate: 5.0000e-04\nEpoch 38/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.2488 - mae: 0.3956 - val_loss: 1.0497 - val_mae: 0.7663 - learning_rate: 5.0000e-04\nEpoch 39/200\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.2528 - mae: 0.3993 - val_loss: 1.0323 - val_mae: 0.7616 - learning_rate: 5.0000e-04\nEpoch 40/200\n\u001b[1m53/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2470 - mae: 0.3917\nEpoch 40: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.2472 - mae: 0.3918 - val_loss: 1.0164 - val_mae: 0.7504 - learning_rate: 5.0000e-04\n\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step\nMAE=16.44, RMSE=21.09, MAPE=28.50%, R²=-0.2237\nSaved all components to pm25_lstm_v1_files\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import joblib\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model\n\nclass PM25Predictor:\n    def __init__(self, model_dir):\n        self.model = load_model(\n            f\"{model_dir}/pm25_lstm_v1.h5\",\n            custom_objects={\"mse\": tf.keras.losses.MeanSquaredError()}\n        )\n        \n        self.scaler = joblib.load(f\"{model_dir}/pm25_lstm_v1_scaler.joblib\")\n        self.metadata = joblib.load(f\"{model_dir}/pm25_lstm_v1_metadata.joblib\")\n\n        self.SEQ_LENGTH = self.metadata['sequence_length']\n        self.FUTURE_DAYS = self.metadata['future_days']\n        self.FEATURE_NAMES = self.metadata['feature_names']\n        self.NUM_FEATURES = self.metadata['num_features']\n\n    def _prepare_features(self, df):\n        df = df.copy()\n        df.index = pd.to_datetime(df.index)\n\n        if 'pm25' not in df.columns:\n            raise ValueError(\"DataFrame must contain a 'pm25' column.\")\n\n        df['day_of_week'] = df.index.dayofweek\n        df['month'] = df.index.month\n        df['quarter'] = df.index.quarter\n        df['is_weekend'] = (df.index.dayofweek >= 5).astype(int)\n        df['hour'] = getattr(df.index, 'hour', 0)\n\n        for lag in [1, 2, 3, 7, 14]:\n            df[f'pm25_lag_{lag}'] = df['pm25'].shift(lag)\n\n        for window in [3, 7, 14]:\n            df[f'pm25_rolling_mean_{window}'] = df['pm25'].rolling(window).mean()\n            df[f'pm25_rolling_std_{window}'] = df['pm25'].rolling(window).std()\n\n        df['pm25_pct_change'] = df['pm25'].pct_change()\n        df['pm25_diff'] = df['pm25'].diff()\n\n        return df.dropna()\n\n    def predict_next(self, df):\n        \"\"\"\n        df: DataFrame with 'pm25' column and datetime index (at least SEQ_LENGTH days of data)\n        \"\"\"\n        df_processed = self._prepare_features(df)\n        df_processed = df_processed[self.FEATURE_NAMES]\n        scaled = self.scaler.transform(df_processed)\n\n        if scaled.shape[0] < self.SEQ_LENGTH:\n            raise ValueError(f\"Need at least {self.SEQ_LENGTH} days of data for prediction\")\n\n        last_seq = scaled[-self.SEQ_LENGTH:]\n        X_input = np.expand_dims(last_seq, axis=0)\n\n        y_pred_scaled = self.model.predict(X_input)\n\n        temp = np.zeros((self.FUTURE_DAYS, self.NUM_FEATURES))\n        temp[:, 0] = y_pred_scaled[0]\n        y_pred_original = self.scaler.inverse_transform(temp)[:, 0]\n\n        last_date = df.index[-1]\n        future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=self.FUTURE_DAYS, freq='D')\n        return pd.DataFrame({\"date\": future_dates, \"predicted_pm25\": y_pred_original})\n\n# ===== Example usage =====\nif __name__ == \"__main__\":\n    predictor = PM25Predictor(\"pm25_lstm_v1_files\")\n\n    df_new = pd.read_csv(\"/kaggle/working/pm25_lstm_v1_files/pm25_lstm_v1_sample_data.csv\", parse_dates=['date'], index_col='date')\n    preds = predictor.predict_next(df_new)\n    print(preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T09:16:15.277905Z","iopub.execute_input":"2025-08-19T09:16:15.278689Z","iopub.status.idle":"2025-08-19T09:16:16.217700Z","shell.execute_reply.started":"2025-08-19T09:16:15.278661Z","shell.execute_reply":"2025-08-19T09:16:16.217045Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n        date  predicted_pm25\n0 2025-08-12       63.152293\n1 2025-08-13       61.921403\n2 2025-08-14       63.325203\n3 2025-08-15       60.879182\n4 2025-08-16       59.051488\n5 2025-08-17       59.729240\n6 2025-08-18       60.903686\n","output_type":"stream"}],"execution_count":6}]}